{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 256 dims\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tensorboard import notebook\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Set the device (GPU or CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Set the hyperparameters\n",
    "batch_size = 32\n",
    "num_epochs = 100\n",
    "learning_rate = 0.0005\n",
    "weight_decay = 0.05  # Increased weight decay\n",
    "save_interval = 1\n",
    "accumulation_steps = 2  # Gradient accumulation steps\n",
    "max_grad_norm = 5.0  # Maximum gradient norm for gradient clipping\n",
    "save_dir = '/content/drive/MyDrive/AutoEncoder-Saves'  # Directory to save checkpoints\n",
    "\n",
    "# Create the save directory if it does not exist\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "# Data transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 64, 64])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import os\n",
    "import torch\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),    # Resize images to 128x128\n",
    "    transforms.ToTensor(),            # Convert images to PyTorch tensors\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize\n",
    "])\n",
    "class marioDataset(Dataset):\n",
    "    def __init__(self, path_dir, num_frames=4, transform=None):\n",
    "        self.dir = path_dir\n",
    "        self.num_frames = num_frames\n",
    "        self.transform = transform\n",
    "        self.image_files = [f for f in os.listdir(path_dir) if os.path.isfile(os.path.join(path_dir, f))]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)//self.num_frames \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load the image from the file\n",
    "        images = []\n",
    "        for i in range(self.num_frames):\n",
    "            img_path = os.path.join(self.dir, self.image_files[idx + i])\n",
    "            with Image.open(img_path).convert(\"RGB\") as image:\n",
    "                image = self.transform(image)\n",
    "                images.append(image)\n",
    "        images = torch.stack(images)\n",
    "        return images\n",
    "    \n",
    "dataset = marioDataset(path_dir='data/mario/',num_frames=1, transform=transform)\n",
    "data_loader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "dataset[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the autoencoder model with residual connections\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, momentum=0.1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels, momentum=momentum)\n",
    "        self.relu = nn.LeakyReLU(0.2)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels, momentum=momentum)\n",
    "        self.skip_connection = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.skip_connection = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride),\n",
    "                nn.BatchNorm2d(out_channels, momentum=momentum)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = self.skip_connection(x)\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, latent_dim=64):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            ResidualBlock(3, 64, stride=2),\n",
    "            ResidualBlock(64, 128, stride=2)\n",
    "        )\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(128 * 16 * 16, latent_dim)\n",
    "        self.fc2 = nn.Linear(latent_dim, 128* 16 * 16)\n",
    "        self.unflatten = nn.Unflatten(1, (128, 16, 16))\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "            ResidualBlock(128, 64, stride=1),\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "            ResidualBlock(64, 64, stride=1),\n",
    "            nn.Conv2d(64, 3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc = self.encoder(x)\n",
    "        latent_space = self.flatten(enc)\n",
    "        latent_space = self.fc1(latent_space)\n",
    "        x = self.fc2(latent_space)\n",
    "        x = self.unflatten(x)\n",
    "        x = self.decoder(x)\n",
    "        return x, latent_space\n",
    "\n",
    "# Perceptual loss using a pre-trained VGG network\n",
    "class PerceptualLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PerceptualLoss, self).__init__()\n",
    "        vgg = torchvision.models.vgg16(pretrained=True).features\n",
    "        self.slice1 = nn.Sequential(*list(vgg.children())[:4]).eval()\n",
    "        for param in self.slice1.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x_vgg, y_vgg = self.slice1(x), self.slice1(y)\n",
    "        loss = nn.functional.l1_loss(x_vgg, y_vgg)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(epoch, model):\n",
    "    model_path = os.path.join(save_dir, f'model_epoch_{epoch + 1}.pth')\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print(f'Model saved at epoch {epoch + 1}')\n",
    "\n",
    "def load_model(model_path):\n",
    "    model = Autoencoder()\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.to(device)\n",
    "    print(f\"Model loaded from {model_path}\")\n",
    "    return model\n",
    "\n",
    "# Laplacian filter to extract high-frequency components\n",
    "laplacian_kernel = torch.tensor([[0, -1, 0], [-1, 4, -1], [0, -1, 0]], dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
    "laplacian_kernel = laplacian_kernel.to(device)\n",
    "\n",
    "def high_pass_filter(img):\n",
    "    batch_size, channels, height, width = img.size()\n",
    "    filtered_img = torch.empty_like(img)\n",
    "    for i in range(channels):\n",
    "        filtered_img[:, i:i+1, :, :] = F.conv2d(img[:, i:i+1, :, :], laplacian_kernel, padding=1)\n",
    "    return filtered_img\n",
    "\n",
    "def check_for_nans(tensor, name):\n",
    "    if torch.isnan(tensor).any():\n",
    "        print(f'NaNs found in {name}')\n",
    "        return True\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Training loop with high-frequency emphasis\n",
    "dataloader = data_loader\n",
    "def train_autoencoder(model, start_epoch=0, num_epochs=100):\n",
    "    criterion = nn.L1Loss()\n",
    "    perceptual_loss = PerceptualLoss().to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    scaler = GradScaler()\n",
    "    writer = SummaryWriter()\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        for i, inputs in enumerate(dataloader):\n",
    "            inputs = inputs.to(device)\n",
    "            inputs = inputs.view(-1, 3, 64, 64).to(device)\n",
    "\n",
    "            with autocast():\n",
    "                reconstructed, _ = model(inputs)\n",
    "                if check_for_nans(reconstructed, 'reconstructed'):\n",
    "                    continue\n",
    "                high_freq_inputs = high_pass_filter(inputs)\n",
    "                high_freq_reconstructed = high_pass_filter(reconstructed)\n",
    "                loss = criterion(reconstructed, inputs) + perceptual_loss(reconstructed, inputs)\n",
    "                high_freq_loss = criterion(high_freq_reconstructed, high_freq_inputs)\n",
    "                loss += high_freq_loss\n",
    "                loss = loss / accumulation_steps\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            if (i + 1) % accumulation_steps == 0:\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            running_loss += loss.item() * accumulation_steps\n",
    "\n",
    "            if i % 10 == 0:\n",
    "                print(f'Epoch {epoch + 1}, Step {i + 1}/{len(dataloader)}, Loss: {loss.item() * accumulation_steps}')\n",
    "                writer.add_scalar('Loss/train', loss.item() * accumulation_steps, epoch * len(dataloader) + i)\n",
    "\n",
    "        avg_loss = running_loss / len(dataloader)\n",
    "        writer.add_scalar('Loss/epoch', avg_loss, epoch)\n",
    "        print(f'Epoch {epoch + 1} finished with avg loss: {avg_loss}')\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            random_idx = np.random.randint(0, len(dataset))\n",
    "            random_image = dataset[random_idx].to(device)\n",
    "            reconstructed_image, latent_space = model(random_image)\n",
    "            plt.imshow(reconstructed_image.squeeze(0).transpose(2,0).transpose(0,1).cpu())\n",
    "            plt.show()\n",
    "            writer.add_image('Original Image', (random_image.squeeze(0) + 1) / 2, epoch)\n",
    "            writer.add_image('Reconstructed Image', (reconstructed_image.squeeze(0) + 1) / 2, epoch)\n",
    "\n",
    "        if (epoch + 1) % save_interval == 0:\n",
    "            save_model(epoch, model)\n",
    "\n",
    "    writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model = Autoencoder().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 6564), started 3:29:57 ago. (Use '!kill 6564' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-723602a969d17e89\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-723602a969d17e89\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Start TensorBoard\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=runs\n",
    "\n",
    "train_autoencoder(model, start_epoch=20, num_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "kill: 6564: No such process\n"
     ]
    }
   ],
   "source": [
    "!kill 6564"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
