{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "def empty_folder(folder):\n",
    "    # Check if the folder exists\n",
    "    if not os.path.exists(folder):\n",
    "        return\n",
    "    \n",
    "    # Get the list of files in the folder\n",
    "    files = os.listdir(folder)\n",
    "    \n",
    "    # Loop through the files and remove them\n",
    "    for file in files:\n",
    "        file_path = os.path.join(folder, file)\n",
    "        os.remove(file_path)\n",
    "\n",
    "def extract_frames(mp4_path, start_frame, end_frame, stride, save_dir='data/mario', size = (64,64)):\n",
    "    # Open the video file\n",
    "    video = cv2.VideoCapture(mp4_path)\n",
    "    empty_folder(save_dir)\n",
    "    # Get the total number of frames in the video\n",
    "    total_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    print(\"Total frames in video:\", total_frames)\n",
    "    # Validate the start and end frame numbers\n",
    "    if start_frame < 0 or start_frame >= total_frames:\n",
    "        raise ValueError(\"Invalid start frame number\")\n",
    "    if end_frame < 0 or end_frame >= total_frames:\n",
    "        raise ValueError(\"Invalid end frame number\")\n",
    "    if start_frame > end_frame:\n",
    "        raise ValueError(\"Start frame number cannot be greater than end frame number\")\n",
    "    \n",
    "    # Validate the stride value\n",
    "    if stride <= 0:\n",
    "        raise ValueError(\"Stride value must be greater than zero\")\n",
    "    \n",
    "    # Set the current frame number to the start frame\n",
    "    current_frame = start_frame\n",
    "    \n",
    "    # List to store the extracted frames\n",
    "    frames = []\n",
    "    \n",
    "    # Loop through the frames and extract the desired frames\n",
    "    while current_frame <= end_frame:\n",
    "        # Set the current frame number\n",
    "        video.set(cv2.CAP_PROP_POS_FRAMES, current_frame)\n",
    "        \n",
    "        # Read the frame\n",
    "        ret, frame = video.read()\n",
    "        \n",
    "        # Check if the frame was read successfully\n",
    "        if not ret:\n",
    "            raise ValueError(\"Error reading frame\")\n",
    "        \n",
    "        # Resize the frame to the specified size\n",
    "        frame = cv2.resize(frame, size)\n",
    "        # Save the frame to the save_dir\n",
    "        cv2.imwrite(os.path.join(save_dir, f\"frame_{current_frame}.jpg\"), frame)\n",
    "        # Append the frame to the list\n",
    "        frames.append(frame)\n",
    "        \n",
    "        # Increment the current frame number by the stride\n",
    "        current_frame += stride\n",
    "    \n",
    "    # Release the video file\n",
    "    video.release()\n",
    "    \n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total frames in video: 69878\n"
     ]
    }
   ],
   "source": [
    "start = 400\n",
    "end = 1000\n",
    "stride = 200\n",
    "frames = extract_frames(\"data/videos/mario_gameplay.mp4\", start, end, stride, size=(64,64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame number: 1000\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "i = 33\n",
    "for i in range(0, len(frames), 1):\n",
    "    clear_output(wait=True)  # Clear the output of the current cell\n",
    "    plt.imshow(frames[i])\n",
    "    print(\"Frame number:\", start + stride * i)\n",
    "    plt.show()\n",
    "    time.sleep(0.03)  # Add a 1-second wait time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "frames = torch.tensor(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([301, 64, 64, 3])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frames.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: '/content/drive/MyDrive/mcdata2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[63], line 40\u001b[0m\n\u001b[0;32m     33\u001b[0m transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[0;32m     34\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mResize((\u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m256\u001b[39m)),\n\u001b[0;32m     35\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[0;32m     36\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mNormalize(mean\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m], std\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m])\n\u001b[0;32m     37\u001b[0m ])\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Load the dataset\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mImageFolder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/content/drive/MyDrive/mcdata2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, pin_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# Define the autoencoder model with residual connections\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\anmol\\anaconda3\\lib\\site-packages\\torchvision\\datasets\\folder.py:328\u001b[0m, in \u001b[0;36mImageFolder.__init__\u001b[1;34m(self, root, transform, target_transform, loader, is_valid_file, allow_empty)\u001b[0m\n\u001b[0;32m    319\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    321\u001b[0m     root: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    326\u001b[0m     allow_empty: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    327\u001b[0m ):\n\u001b[1;32m--> 328\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mIMG_EXTENSIONS\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    333\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_transform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_transform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    334\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_valid_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    335\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_empty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_empty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    337\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples\n",
      "File \u001b[1;32mc:\\Users\\anmol\\anaconda3\\lib\\site-packages\\torchvision\\datasets\\folder.py:149\u001b[0m, in \u001b[0;36mDatasetFolder.__init__\u001b[1;34m(self, root, loader, extensions, transform, target_transform, is_valid_file, allow_empty)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    140\u001b[0m     root: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    146\u001b[0m     allow_empty: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    147\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    148\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(root, transform\u001b[38;5;241m=\u001b[39mtransform, target_transform\u001b[38;5;241m=\u001b[39mtarget_transform)\n\u001b[1;32m--> 149\u001b[0m     classes, class_to_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_classes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    150\u001b[0m     samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_dataset(\n\u001b[0;32m    151\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot,\n\u001b[0;32m    152\u001b[0m         class_to_idx\u001b[38;5;241m=\u001b[39mclass_to_idx,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    155\u001b[0m         allow_empty\u001b[38;5;241m=\u001b[39mallow_empty,\n\u001b[0;32m    156\u001b[0m     )\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader \u001b[38;5;241m=\u001b[39m loader\n",
      "File \u001b[1;32mc:\\Users\\anmol\\anaconda3\\lib\\site-packages\\torchvision\\datasets\\folder.py:234\u001b[0m, in \u001b[0;36mDatasetFolder.find_classes\u001b[1;34m(self, directory)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_classes\u001b[39m(\u001b[38;5;28mself\u001b[39m, directory: Union[\u001b[38;5;28mstr\u001b[39m, Path]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[List[\u001b[38;5;28mstr\u001b[39m], Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m]]:\n\u001b[0;32m    208\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Find the class folders in a dataset structured as follows::\u001b[39;00m\n\u001b[0;32m    209\u001b[0m \n\u001b[0;32m    210\u001b[0m \u001b[38;5;124;03m        directory/\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;124;03m        (Tuple[List[str], Dict[str, int]]): List of all classes and dictionary mapping each class to an index.\u001b[39;00m\n\u001b[0;32m    233\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind_classes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\anmol\\anaconda3\\lib\\site-packages\\torchvision\\datasets\\folder.py:41\u001b[0m, in \u001b[0;36mfind_classes\u001b[1;34m(directory)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_classes\u001b[39m(directory: Union[\u001b[38;5;28mstr\u001b[39m, Path]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[List[\u001b[38;5;28mstr\u001b[39m], Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m]]:\n\u001b[0;32m     37\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Finds the class folders in a dataset.\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \n\u001b[0;32m     39\u001b[0m \u001b[38;5;124;03m    See :class:`DatasetFolder` for details.\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m     classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(entry\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscandir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m entry\u001b[38;5;241m.\u001b[39mis_dir())\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m classes:\n\u001b[0;32m     43\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find any class folder in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdirectory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: '/content/drive/MyDrive/mcdata2'"
     ]
    }
   ],
   "source": [
    "#@title 256 dims\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tensorboard import notebook\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Set the device (GPU or CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Set the hyperparameters\n",
    "batch_size = 32\n",
    "num_epochs = 100\n",
    "learning_rate = 0.0005\n",
    "weight_decay = 0.05  # Increased weight decay\n",
    "save_interval = 1\n",
    "accumulation_steps = 2  # Gradient accumulation steps\n",
    "max_grad_norm = 5.0  # Maximum gradient norm for gradient clipping\n",
    "save_dir = 'weights'  # Directory to save checkpoints\n",
    "\n",
    "# Create the save directory if it does not exist\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "# Data transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# Load the dataset\n",
    "dataset = ImageFolder(root='data/mario', transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "# Define the autoencoder model with residual connections\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, momentum=0.1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels, momentum=momentum)\n",
    "        self.relu = nn.LeakyReLU(0.2)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels, momentum=momentum)\n",
    "        self.skip_connection = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.skip_connection = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride),\n",
    "                nn.BatchNorm2d(out_channels, momentum=momentum)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = self.skip_connection(x)\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            ResidualBlock(3, 64, stride=2),\n",
    "            ResidualBlock(64, 128, stride=2),\n",
    "            ResidualBlock(128, 256, stride=2),\n",
    "        )\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(64 * 32 * 32, 64)\n",
    "        self.fc2 = nn.Linear(64, 64 * 32 * 32)\n",
    "        self.unflatten = nn.Unflatten(1, (64, 32, 32))\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "            ResidualBlock(256, 128, stride=1),\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "            ResidualBlock(128, 64, stride=1),\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "            ResidualBlock(64, 64, stride=1),\n",
    "            nn.Conv2d(64, 3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc = self.encoder(x)\n",
    "        latent_space = self.flatten(enc)\n",
    "        latent_space = self.fc1(latent_space)\n",
    "        x = self.fc2(latent_space)\n",
    "        x = self.unflatten(x)\n",
    "        x = self.decoder(x)\n",
    "        return x, latent_space\n",
    "\n",
    "# Perceptual loss using a pre-trained VGG network\n",
    "class PerceptualLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PerceptualLoss, self).__init__()\n",
    "        vgg = torchvision.models.vgg16(pretrained=True).features\n",
    "        self.slice1 = nn.Sequential(*list(vgg.children())[:4]).eval()\n",
    "        for param in self.slice1.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x_vgg, y_vgg = self.slice1(x), self.slice1(y)\n",
    "        loss = nn.functional.l1_loss(x_vgg, y_vgg)\n",
    "        return loss\n",
    "\n",
    "def save_model(epoch, model):\n",
    "    model_path = os.path.join(save_dir, f'model_epoch_{epoch + 1}.pth')\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print(f'Model saved at epoch {epoch + 1}')\n",
    "\n",
    "def load_model(model_path):\n",
    "    model = Autoencoder()\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.to(device)\n",
    "    print(f\"Model loaded from {model_path}\")\n",
    "    return model\n",
    "\n",
    "# Laplacian filter to extract high-frequency components\n",
    "laplacian_kernel = torch.tensor([[0, -1, 0], [-1, 4, -1], [0, -1, 0]], dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
    "laplacian_kernel = laplacian_kernel.to(device)\n",
    "\n",
    "def high_pass_filter(img):\n",
    "    batch_size, channels, height, width = img.size()\n",
    "    filtered_img = torch.empty_like(img)\n",
    "    for i in range(channels):\n",
    "        filtered_img[:, i:i+1, :, :] = F.conv2d(img[:, i:i+1, :, :], laplacian_kernel, padding=1)\n",
    "    return filtered_img\n",
    "\n",
    "def check_for_nans(tensor, name):\n",
    "    if torch.isnan(tensor).any():\n",
    "        print(f'NaNs found in {name}')\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# Training loop with high-frequency emphasis\n",
    "def train_autoencoder(model, start_epoch=0):\n",
    "    criterion = nn.L1Loss()\n",
    "    perceptual_loss = PerceptualLoss().to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    scaler = GradScaler()\n",
    "    writer = SummaryWriter()\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        for i, (inputs, _) in enumerate(dataloader):\n",
    "            inputs = inputs.to(device)\n",
    "\n",
    "            with autocast():\n",
    "                reconstructed, _ = model(inputs)\n",
    "                if check_for_nans(reconstructed, 'reconstructed'):\n",
    "                    return\n",
    "                high_freq_inputs = high_pass_filter(inputs)\n",
    "                high_freq_reconstructed = high_pass_filter(reconstructed)\n",
    "                loss = criterion(reconstructed, inputs) + perceptual_loss(reconstructed, inputs)\n",
    "                high_freq_loss = criterion(high_freq_reconstructed, high_freq_inputs)\n",
    "                loss += high_freq_loss\n",
    "                loss = loss / accumulation_steps\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            if (i + 1) % accumulation_steps == 0:\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            running_loss += loss.item() * accumulation_steps\n",
    "\n",
    "            if i % 10 == 0:\n",
    "                print(f'Epoch {epoch + 1}, Step {i + 1}/{len(dataloader)}, Loss: {loss.item() * accumulation_steps}')\n",
    "                writer.add_scalar('Loss/train', loss.item() * accumulation_steps, epoch * len(dataloader) + i)\n",
    "\n",
    "        avg_loss = running_loss / len(dataloader)\n",
    "        writer.add_scalar('Loss/epoch', avg_loss, epoch)\n",
    "        print(f'Epoch {epoch + 1} finished with avg loss: {avg_loss}')\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            random_idx = np.random.randint(0, len(dataset))\n",
    "            random_image = dataset[random_idx][0].unsqueeze(0).to(device)\n",
    "            reconstructed_image, latent_space = model(random_image)\n",
    "            writer.add_image('Original Image', (random_image.squeeze(0) + 1) / 2, epoch)\n",
    "            writer.add_image('Reconstructed Image', (reconstructed_image.squeeze(0) + 1) / 2, epoch)\n",
    "\n",
    "        if (epoch + 1) % save_interval == 0:\n",
    "            save_model(epoch, model)\n",
    "\n",
    "    writer.close()\n",
    "\n",
    "# Start TensorBoard\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=runs\n",
    "\n",
    "# Initialize and train the model\n",
    "model = Autoencoder().to(device)\n",
    "train_autoencoder(model)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
